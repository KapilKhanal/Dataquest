{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by splitting the data set into 2 nearly equivalent halves.\n",
    "\n",
    "When splitting the data set, don't forget to set a copy of it using .copy() to ensure you don't get any unexpected results later on. If you run the code locally in Jupyter Notebook or Jupyter Lab without .copy(), you'll notice what is known as a SettingWithCopy Warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dc_listings = pd.read_csv(\"dc_airbnb.csv\")\n",
    "stripped_commas = dc_listings['price'].str.replace(',', '')\n",
    "stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "dc_listings['price'] = stripped_dollars.astype('float')\n",
    "\n",
    "shuffled_index = np.random.permutation(dc_listings.index)\n",
    "dc_listings = dc_listings.reindex(shuffled_index)\n",
    "\n",
    "split_one = dc_listings.iloc[0:1862].copy()\n",
    "split_two = dc_listings.iloc[1862:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - train a k-nearest neighbors model on the first half,\n",
    " - test this model on the second half,\n",
    " - train a k-nearest neighbors model on the second half,\n",
    " - test this model on the first half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.2408585628993 131.22316287438863 147.23201071864395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_one = split_one\n",
    "test_one = split_two\n",
    "train_two = split_two\n",
    "test_two = split_one\n",
    "\n",
    "# First half\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(train_one[[\"accommodates\"]], train_one[\"price\"])\n",
    "test_one[\"predicted_price\"] = model.predict(test_one[[\"accommodates\"]])\n",
    "iteration_one_rmse = mean_squared_error(test_one[\"price\"], test_one[\"predicted_price\"])**(1/2)\n",
    "\n",
    "# Second half\n",
    "model.fit(train_two[[\"accommodates\"]], train_two[\"price\"])\n",
    "test_two[\"predicted_price\"] = model.predict(test_two[[\"accommodates\"]])\n",
    "iteration_two_rmse = mean_squared_error(test_two[\"price\"], test_two[\"predicted_price\"])**(1/2)\n",
    "\n",
    "avg_rmse = np.mean([iteration_two_rmse, iteration_one_rmse])\n",
    "\n",
    "print(iteration_one_rmse, iteration_two_rmse, avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Cross Fold Validation\n",
    "\n",
    "If we average the two RMSE values from the last step, we get an RMSE value of approximately 128.96. Holdout validation is actually a specific example of a larger class of validation techniques called k-fold cross-validation. While holdout validation is better than train/test validation because the model isn't repeatedly biased towards a specific subset of the data, both models that are trained only use half the available data. K-fold cross validation, on the other hand, takes advantage of a larger proportion of the data during training while still rotating through different subsets of the data to avoid the issues of train/test validation.\n",
    "\n",
    "Here's the algorithm from k-fold cross validation:\n",
    "\n",
    " - splitting the full dataset into k equal length partitions.\n",
    " - selecting k-1 partitions as the training set and\n",
    " - selecting the remaining partition as the test set\n",
    " - training the model on the training set.\n",
    " - using the trained model to predict labels on the test fold.\n",
    " - computing the test fold's error metric.\n",
    " - repeating all of the above steps k-1 times, until each partition has been used as the test set for an iteration.\n",
    " - calculating the mean of the k error values.\n",
    "\n",
    "Holdout validation is essentially a version of k-fold cross validation when k is equal to 2. Generally, 5 or 10 folds is used for k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0    745\n",
      "2.0    745\n",
      "1.0    745\n",
      "4.0    744\n",
      "3.0    744\n",
      "Name: fold, dtype: int64\n",
      "\n",
      " Num of missing values:  0\n"
     ]
    }
   ],
   "source": [
    "dc_listings.loc[dc_listings.index[0:745], \"fold\"] = 1\n",
    "dc_listings.loc[dc_listings.index[745:1490], \"fold\"] = 2\n",
    "dc_listings.loc[dc_listings.index[1490:2234], \"fold\"] = 3\n",
    "dc_listings.loc[dc_listings.index[2234:2978], \"fold\"] = 4\n",
    "dc_listings.loc[dc_listings.index[2978:3723], \"fold\"] = 5\n",
    "\n",
    "print(dc_listings['fold'].value_counts())\n",
    "print(\"\\n Num of missing values: \", dc_listings['fold'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.74291626020474\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Training\n",
    "model = KNeighborsRegressor()\n",
    "train_iteration_one = dc_listings[dc_listings[\"fold\"] != 1]\n",
    "test_iteration_one = dc_listings[dc_listings[\"fold\"] == 1].copy()\n",
    "model.fit(train_iteration_one[[\"accommodates\"]], train_iteration_one[\"price\"])\n",
    "\n",
    "# Predicting\n",
    "labels = model.predict(test_iteration_one[[\"accommodates\"]])\n",
    "test_iteration_one[\"predicted_price\"] = labels\n",
    "iteration_one_mse = mean_squared_error(test_iteration_one[\"price\"], test_iteration_one[\"predicted_price\"])\n",
    "iteration_one_rmse = iteration_one_mse ** (1/2)\n",
    "\n",
    "print(iteration_one_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first iteration, we achieved an RMSE value of roughly 124. Let's calculate the RMSE values for the remaining iterations. To make the iteration process easier, let's wrap the code we wrote in the previous screen in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137.74291626020474, 120.65465299187844, 155.25353977575915, 139.64903454540016, 86.51907746330096]\n",
      "127.96384420730867\n"
     ]
    }
   ],
   "source": [
    "fold_ids = [1,2,3,4,5]\n",
    "def train_and_validate(df, folds):\n",
    "    fold_rmses = []\n",
    "    for fold in folds:\n",
    "        # Train\n",
    "        model = KNeighborsRegressor()\n",
    "        train = df[df[\"fold\"] != fold]\n",
    "        test = df[df[\"fold\"] == fold].copy()\n",
    "        model.fit(train[[\"accommodates\"]], train[\"price\"])\n",
    "        # Predict\n",
    "        labels = model.predict(test[[\"accommodates\"]])\n",
    "        test[\"predicted_price\"] = labels\n",
    "        mse = mean_squared_error(test[\"price\"], test[\"predicted_price\"])\n",
    "        rmse = mse**(1/2)\n",
    "        fold_rmses.append(rmse)\n",
    "    return(fold_rmses)\n",
    "\n",
    "rmses = train_and_validate(dc_listings, fold_ids)\n",
    "print(rmses)\n",
    "avg_rmse = np.mean(rmses)\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136.13196696 121.82150811 155.53378001 162.76730372  99.306953  ]\n",
      "135.11230236229338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "kf = KFold(5, shuffle=True, random_state=1)\n",
    "model = KNeighborsRegressor()\n",
    "mses = cross_val_score(model, dc_listings[[\"accommodates\"]], dc_listings[\"price\"], scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "rmses = np.sqrt(np.absolute(mses))\n",
    "avg_rmse = np.mean(rmses)\n",
    "\n",
    "print(rmses)\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right k value when performing k-fold cross validation is more of an art and less of a science. \n",
    "\n",
    "As we discussed earlier in the mission, a k value of 2 is really just holdout validation. \n",
    "\n",
    "On the other end, setting k equal to n (the number of observations in the data set) is known as leave-one-out cross validation, or LOOCV for short. \n",
    "\n",
    "Through lots of trial and error, data scientists have converged on 10 as the standard k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 folds:  avg RMSE:  127.06965345727298 std RMSE:  15.846517938155053\n",
      "5 folds:  avg RMSE:  135.11230236229338 std RMSE:  22.984968219940907\n",
      "7 folds:  avg RMSE:  128.7310175398072 std RMSE:  27.221295979788373\n",
      "9 folds:  avg RMSE:  134.61697548752926 std RMSE:  36.9612947796052\n",
      "10 folds:  avg RMSE:  126.40912746156773 std RMSE:  37.14777750653743\n",
      "11 folds:  avg RMSE:  133.0868875025243 std RMSE:  32.81227878239363\n",
      "13 folds:  avg RMSE:  129.41850536672487 std RMSE:  37.0369322754536\n",
      "15 folds:  avg RMSE:  132.96768055999738 std RMSE:  36.432734176987644\n",
      "17 folds:  avg RMSE:  130.22270090831245 std RMSE:  42.9675639583121\n",
      "19 folds:  avg RMSE:  125.54727462548044 std RMSE:  40.971624884633066\n",
      "21 folds:  avg RMSE:  124.9508658320053 std RMSE:  42.760538966024555\n",
      "23 folds:  avg RMSE:  126.71180034548591 std RMSE:  44.13210992839968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_folds = [3, 5, 7, 9, 10, 11, 13, 15, 17, 19, 21, 23]\n",
    "\n",
    "for fold in num_folds:\n",
    "    kf = KFold(fold, shuffle=True, random_state=1)\n",
    "    model = KNeighborsRegressor()\n",
    "    mses = cross_val_score(model, dc_listings[[\"accommodates\"]], dc_listings[\"price\"], scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "    rmses = np.sqrt(np.absolute(mses))\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    std_rmse = np.std(rmses)\n",
    "    print(str(fold), \"folds: \", \"avg RMSE: \", str(avg_rmse), \"std RMSE: \", str(std_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
